kubectl get nodes
kubectl get nodes -o wide
kubectl get nodes -o json
kubectl get nodes -o yaml

kubectl describe nodes
kubectl describe nodes master
kubectl describe nodes worker1

#Kubernetes Objects list (Search google)

kubectl api-resources

kubectl get namespaces
kubectl get ns
kubectl create ns production
kubectl describe namespaces production
kubectl delete namespaces production


kubectl get pods
kubectl get pods -o wide
kubectl get pods -A
kubectl get pods -n kube-system
kubectl get pods -n kube-flannel

#Live watch pods
watch kubectl get pods
watch kubectl get pods <container name>
kubectl get pods -w



kubectl run -h
kubectl run pod1 --image nginx
curl 10.244.1.14
curl http://10.244.1.14
kubectl describe pods pod1
kubectl logs pod/pod1


kubectl delete pods pod1                  # Delete spacefic pod
kubectl delete pods --all                 # All pods delete
kubectl delete pods --all --force         # Force All pods delete

kubectl create namespace ns1
kubectl run pod1 --image nginx --namespace ns1
kubectl get pods -n ns1
kubectl delete pods -n ns1 pod1
kubectl delete pods -n ns1 --all


kubectl get pods -n kube-system | wc -l


kubectl explain namespace
kubectl explain pod
kubectl explain replicaset


#Create a pod using yaml
 kubectl apply -f single.yaml --dry-run
 kubectl apply -f single.yaml

 kubectl get pods
 kubectl run pod2 --image nginx --dry-run=client -o yaml
# Output
----------------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod2
  name: pod2
spec:
  containers:
  - image: nginx
    name: pod2
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
------------------------------
 kubectl set image pod/<pod name> <container name> doc1=httpd
 kubectl exec -it multicontainer /bin/bash   #login container
 kubectl exec -it multicontainer /bin/sh     #login container


#Deploy Replicas
 kubectl create deployment demodep1 --image nginx --replicas 10
 kubectl delete deployments.apps <deployment name>
 kubectl scale deployment <deployment name> --replicas 0                    #Scale out 
 kubectl scale deployment <deployment name> --replicas 10                   #Scale In


#Edit pods
 kubectl edit pod <pod name>   #Change only image but not change name or not add container image.but output yaml file provide
 kubectl describe pod pod1 | grep -i image
 kubectl describe pod pod1 | more 
 kubectl set image pods/pod1 pod1=nginx


#kubectl apply/create/replace 
 kubectl create -f <yaml file>             # newcreate resources
 kubectl apply -f <yaml file>              # changeable yaml file info update pod
 kubectl replace -f <yaml file>            # replace pod
 kubectl replace -f <yaml file> --force    # delete+new create 


#Login container shells or run command
 kubectl exec pods/pod1 -- pwd
 kubectl exec pods/pod1 -- date
 kubectl exec pods/pod1 -- hostname
 kubectl exec pods/<podname> -- "command"
 kubectl exec pods/<podname> -c <containername> -- "command"
 #Example
   kubectl exec -it pods/pod1 -- /bin/bash
   kubectl exec -it pods/pod1 -- /bin/sh 

#copy contents from host to pod
 kubectl cp <source> <pod-name>:<destination>
 kubectl cp <source> <pod-name>:<destination> -c <containername>
 #Example
  kubectl cp index.html multicontainer:/usr/share/nginx/html/
  kubectl cp index.html multicontainer:/tmp -c redis


#copy contents from pod to host
 kubectl cp <source> <pod-name>:<sourcefilewithpath> <destination-must-with-file-name>
 kubectl cp <source> <pod-name>:<sourcefilewithpath> <destination-must-with-file-name> -c <containername>
#Example
 kubectl cp -n default multicontainer:/usr/share/nginx/html/index.html /tmp/abc
 kubectl cp -n default multicontainer:/usr/share/nginx/html/index.html ./index.html
 kubectl cp multicontainer:/usr/share/nginx/html/index.html ./index.html
 kubectl cp multicontainer:/usr/share/nginx/html/index.html ./index.html -c doc2
 kubectl cp dhaka/ multicontainer:/tmp   #Directoy copy


#Logs 
 kubectl logs pods <podname>
 kubectl logs pods <podname> -c <containername>
 kubectl logs pods <podname -c <containername> --since=1h
 kubectl logs pods <podname -c <containername> --tail 10
 kubectl logs pods <podname> -c <containername> -f


#Pod Restart Policy 
 #type of restart policy
 kubectl run always --imgae ubuntu #Default policy always
 kubectl run never --image ubuntu --restart Never  #No Restart 
 kubectl run onfailure --image ubuntu --restart OnFailure   #Just failed time restart this pods
 kubectl get pods always -o yaml | grep -i restart

  
#Side  car container

#init container
apiVersion: v1
kind: Pod
metadata:
 name: pod1
spec:
containers:
 - name: container1
   image: nginx
initContainers:
 - name: containers2
   image: redis


--------------------------

apiVersion: v1
kind: Pod
metadata:
 name: pod1
spec:
containers:
 - name: container1
   image: nginx
initContainers:
 - name: containers2
   image: ubuntu



#Service + Label in Kubernetes
 #type of Service
  1. ClusterIP
  2. NodePort
  3. LoadBalancer
  4. ExternalName

#ClusterIP 
kubectl get service #Build in service
kubectl create service clusterip svc1 --tcp 80
kubectl describe service <service name>
kubectl edit service <service name>  #Edit Label
kubectl get service --show-labels    # Show label
kubectl edit pod <pod name>          #Edit Pod
kubectl label pod webapp2 app=svc1   #Set label
kubectl label --overwrite pods <pod_name> key=new_value #overwrite label
kubectl create service clusterip --clusterip 10.97.101.55 --tcp 81:80 svc2

kubectl run pod1 --image nginx --port 80 --expose   #expose (create service and label set)
kubectl expose pod web1 --port 80
kubectl expose pod web1 --port 81 --target-port 80
kubectl get service web1 -o yaml
kubectl run web3 --image nginx --labels run=web1
kubectl create service clusterip svc1 --tcp 80 -o yaml --dry-run=clientm > svc1.yaml

#Example
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: svc1
  name: svc1
spec:
  ports:
  - name: "80"
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: svc1
  type: ClusterIP
status:
  loadBalancer: {}


#NodePort (NodePorts in Kubernetes is 30,000â€“32,767)
kubectl create service nodeport nps1 --tcp 80
kubectl create service nodeport nps1 --tcp 80 -o yaml --dry-run=client > nodeports.yaml
#Example
----------
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: nps1
  name: nps1
spec:
  ports:
  - name: "80"
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nps1
  type: NodePort
status:
  loadBalancer: {}

----------


kubectl create service nodeport nps1 --tcp 80 --node-port 30100
kubectl create service nodeport nps1 --tcp 80 --node-port 30100 -o yaml --dry-run=client > nodeports.yaml
#Example
---------
apiVersion: v2
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: nps1
  name: nps1
spec:
  ports:
  - name: "80"
    nodePort: 30100
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nps1
  type: NodePort
status:
  loadBalancer: {}
-------------------

#LoadBalancer


#ExternalName
kubectl create service externalname ext1 --external-name abc.com --tcp 80
kubectl create service externalname ext1 --external-name abc.com --tcp 80 -o yaml --dry-run=client
kubectl create service externalname ext1 --external-name <Ip address> --tcp 80
kubectl create service externalname ext1 --external-name <Ip address> --tcp 80 -o yaml --dry-run=client

#Example
----------
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: ext1
  name: ext1
spec:
  externalName: abc.com
  ports:
  - name: "80"
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: ext1
  type: ExternalName
status:
  loadBalancer: {}

------------------------

#Show Network
iptables -t nat -L  #Show ip tables
iptables -t nat -nL
iptables -t nat -nL | grep -i 80


#High avilablity and scaleablity in Kubernetes
#ReplicationController
#Example
-----------------
apiVersion: v1
kind: ReplicationController
metadata:
  name: demorc1
spec:
  replicas: 2
  template:
    metadata:
      name: pod1
      labels:
        app: web
    spec:
      containers:
        - name: doc1
          image: nginx
-------------------------------

kubectl apply -f rc.yaml
kubectl get replicationcontrollers
kubectl get replicationcontrollers <replicationcontroller name>
kubectl delete replicationcontrollers demorc1
kubectl scale replicationcontrollers demorc1 --replicas 5
kubectl edit replicationcontrollers <replication name>
kubectl explain replicationcontroller
kubectl describe replicationcontrollers demorc1
kubectl set image replicationcontrollers/demorc1 doc=httpd
kubectl expose replicationcontrollers demorc1
kubectl label <node or pod name> worker1 size- (;abel key-)


#Note: Limition labels, old technology,runnig pod not change image.

#ReplicaSet 
kubectl explain replicaset
kubectl describe replicaset.apps demors1
kubectl apply -f rs.yaml
kubectl get replicaset <replicaset name>
kubectl delete replicaset demors1
kubectl scale replicaset demors1 --replicas 5
kubectl edit replicaset <replication name>
kubectl set image replicaset/demors1 doc=httpd
kubectl expose replicaset demors1
kubectl label <node or pod name> worker1 size- (;abel key-)


#Example
---------------
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: demors1
spec:
  replicas: 2
  selector:
    matchLabels:
      dep: cse
      batch: sixth
  template:
    metadata:
      labels:
        dep: cse
        batch: sixth
    spec:
      containers:
        - name: doc1
          image: nginx
-------------------------

#Note: Running change image not posssible, rollback not possible.



#Deployment (automatic create replicaset)
kubectl create deployment --image nginx --replicas 3 
kubectl create deployment --image nginx --replicas 3 -o yaml --dry-run=client > dep.yaml
kubectl explain deployment
kubectl describe deployment.apps demors1
kubectl apply -f dep.yaml
kubectl get deployment <replicaset name>
kubectl delete deployment demors1
kubectl scale deployment demors1 --replicas 5
kubectl edit deployment <replication name>
kubectl set image deployment demors1 doc=httpd
kubectl expose deployment demors1
kubectl get deployments.apps <deployment name>
kubectl describe replicasets.apps <deployment name>
kubectl expose deployment dep1 --port 80
kubectl set image deploy/dep1 nginx=httpd
kubectl rollout history deployment <deployment name>
kubectl rollout history deployment dep1 --revision 1
kubectl rollout undo deployment dep1
kubectl rollout undo deployment dep1 --to-revision 1
kubectl label <node or pod name> worker1 size- (;abel key-)





#Example
------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demors1
spec:
  replicas: 2
  selector:
    matchLabels:
      dep: cse
      batch: sixth
  template:
    metadata:
      labels:
        dep: cse
        batch: sixth
    spec:
      containers:
        - name: doc1
          image: nginx
------------------------------


#Scheduling pod
kubectl get pods --namespace kube-system
cd /etc/kubernetes/manifests/
ls 
mv kube-scheduler.yaml /root/
mv kube-scheduler.yaml /etc/kubernetes/manifests/
kubectl label <node or pod name> worker1 size- (;abel key-)


#Example
----------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: manual
  name: manual
spec:
  nodeName: worker1
  containers:
  - image: nginx
    name: manual
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
------------------------




#Node Selector 
kubectl get nodes --show-labels
kubectl get nodes --show-labels worker1
kubectl label node worker1 disk=nvme
kubectl label node worker2 disk=ssd 
kubectl get nodes --selector disk=nvme
kubectl get nodes --selector disk=ssd
kubectl label <node or pod name> worker1 size- (;abel key-)

#Example
----------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nodeselector
  name: nodeselector
spec:
  nodeSelector:
    disk: nvme
  containers:
  - image: nginx
    name: manual
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
------------------------


#Affinity
kubectl label <node or pod name> worker1 size- (;abel key-)

#Example:nodeAffinityRequired
-----------------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: web
  name: web
spec:
  affinity:
   nodeAffinity:
     requiredDuringSchedulingIgnoredDuringExecution:
       nodeSelectorTerms:
         - matchExpressions:
             - key: size
               operator: In   #operator type (In, NotIn, Exists, DoesNotExist, Gt, Lt)
               values:
                - small
                - medium
  containers:
  - image: nginx
    name: web 
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
-------------------------------

#Example:nodeAffinityPreferred
--------------------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: web
  name: web
spec:
  affinity:
   nodeAffinity:
     preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: size
            operator: In
            values:
            - large 
  containers:
  - image: nginx
    name: web 
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
------------------------------


#Pod Affinity

#Example
----------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: web
  name: web
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: kubernetes.io/hostname 
  containers:
  - image: nginx
    name: web 
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
---------------------------------

#Taint and tolerations
kubectl describe nodes master | grep -i taint -C 5
kubectl describe nodes worker1 | grep -i taint
kubectl taint node master node-role.kubernetes.io/control-plane:NoSchedule-
kubectl taint node master node-role.kubernetes.io/control-plane:NoSchedule
kubectl taint node master node-role.kubernetes.io/control-plane:NoExcute
kubectl taint node master node-role.kubernetes.io/control-plane:PreferNoSchedule
kubectl run pod1 --image nginx -o yaml --dry-run=client > tolerations.yaml
vi tolerations.yaml
kubectl taint node worker1 dep=cse:NoSchedule


#Example
-------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod1
  name: pod1
spec:
  tolerations:
    - key: deb
      operator: Eual
      value: cse
      effect: NoSchedule
  containers:
  - image: nginx
    name: pod1
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
------------------------------


#StaticPod 
kubectl run staticpod --image nginx -o yaml --dry-run=client > staticpod.yaml
scp staticpod.yaml azuser@worker1:
cd /etc/kubernetes/manifests
mv /root/staticpod.yaml ./
kubectl exec -it staticpod-worker1 -- /bin/bash
ps -aux | grep -i kubelet 
vi /var/lib/kubelet/config.yaml
mv /etc/kubernetes/manifests kube-Scheduler.yaml /opt/
mv /opt/kube-scheduler.yaml /etc/kubernetes/manifests/


#DaemonSet
kubectl get ns
kubectl explain daemonsets.apps
kubectl api-resources | grep -i daemon
kubectl get ds
kubectl get daemonsets.apps
kubectl get daemonsets.apps -n kube-system
kubectl edit daemonsets.apps -n kube-system
kubectl get rs -n kube-system kube-proxy
kubectl apply -f daemonsert.yaml
kubectl delete daemonsets.apps daemonset


#Example
--------------
apiVersion: apps/v1
kind: DaemonSet
metadata:
  creationTimestamp: null
  labels:
    app: daemonset
  name: daemonset
spec:
  selector:
    matchLabels:
      app: daemonset
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: daemonset
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
----------------------------------


#metrics server (Note: Update 15 se maximum)
kubectl top node
kubectl top pod 
wget https://raw.githubusercontent.com/pythianarora/total-practice/master/sample-kubernetes-code/metrics-server.yaml
kubectl get pod --namespace kube-system
kubectl get ns 
kubectl apply -f metrics-server.yaml
kubectl get pods -A
kubectl get pods --namespace kube-system
kubectl top node
kubectl top node <node name>
kubectl top pod 
kubectl top pod <pod name>
kubectl top node --sort-by cpu
kubectl top node --sort-by memory
kubectl top pod --selector pod=pod 
kubectl top pod --sort-by cpu > /tmp/cpu.list


#Worker node work load 
dd if=/dev/zero of=/dev/null&
dd if=/dev/zero of=/dev/null bs=1G&  #backround run
dd if=/dev/zero of=/dev/null bs=1G
jobs
fg 1
fg 2


#Limits 
nproc
lscpu
cat /proc/cpuinfo
kubectl run pod1 --image nginx
kubectl describe pod pod1
kubectl exec -it pod1 -- /bin/bash
kubectl run pod2 --image nginx -o yaml --dry-run=client > limit.yaml


#Example
------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod2
  name: pod2
spec:
  containers:
  - image: nginx
    name: pod2
    resources:
     requests:
       cpu: 256m
       memory: 256Mi
     limits:
       cpu: 500m
       memory: 500Mi
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
----------------------------



#LimitRange  (pod)
kubectl explain limitrange 
kubectl apply -f default-cpu-memory-limit.yaml
kubectl get limitranges cpu-limit-range
kubectl describe limitranges cpu-limit-range 
kubectl apply -f default-cpu-memory-limit.yaml --namespace <namespace name>
kubectl get limitranges --namespaces <namespace nam>
kubectl get limitranges

#Example
---------------
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 1
      memory: 200Mi
    defaultRequest:
      cpu: 0.5
      memory: 100Mi
    type: Container
---------------------


#LimitRange  (namespace) resources quotas
kubectl get resourcequotas
kubectl apply -f limitrange.yaml
kubectl get limitranges
kubectl describe limitranges <limitrange name>
kubectl describe pod pod1 | grep -i limit -C 10
kubectl exec -it pod1 -- /bin/sh
dd if=/dev/zero of=/dev/null
kubectl delete pod pod1 --force --grace-period=0


#Example
----------------
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-resource-constraint
spec:
  limits:
  - default: # this section defines default limits
      cpu: 200m
    defaultRequest: # this section defines default requests
      cpu: 200m
    max: # max and min define the limit range
      cpu: "200m"
    min:
      cpu: 200m
    type: Container
--------------------------------------


#Horizantail pod scaling (HPA)
kubectl create deployment dep1 --image nginx
kubectl exec -it pods/dep1-64cd8df7fd-8hf6m -- /bin/sh
dd if=/dev/zero of=/dev/null
kubectl describe pods dep1-64cd8df7fd-8hf6m | grep -i cpu
kubectl api-resources | grep  hpa
kubectl explain hpa
kubectl get horizontalpodautoscalers.autoscaling
kubectl get hpa 
kubectl autoscale deployment dep1 --max 3 --min 1 --cpu-percent 70
kubectl describe hpa 


#Example (HPA memory)
-----------------
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: "2024-12-01T04:15:38Z"
  name: dep1
  namespace: default
spec:
  maxReplicas: 3
  metrics:
  - resource:
      name: memory
      target:
        averageUtilization: 80
        type: Utilization
    type: Resource
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: dep1
--------------------------------

#Environment Variable
whereis ls 
export name=reja
unset name
kubectl exec -it pod1 -- env 
kubectl run pod1 --image nginx --env batch=cka3
kubectl run pod1 --image nginx --env batch=cka3 --env name=reja
kubectl run pod1 --image nginx --env batch=cka3 --env name=reja -o yaml --dry-run=client > env.yaml

#Example
------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod1
  name: pod1
spec:
  containers:
  - env:
    - name: batch
      value: cka3
    - name: name
      value: reja
    image: nginx
    name: pod1
-------------------------------

kubectl run db1 --image mysql --env MYSQL_ROOT_PASSWORD=cka321
kubectl run db2 --image mysql --eve MYSQL_ROOT_PASSWORD=cka321 -o yaml --dry-run=client > db2.yaml

#Example
----------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: db2
  name: db2
spec:
  containers:
  - args:
    - MYSQL_ROOT_PASSWORD=cka321
    image: mysql
    name: db2
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
-----------------------

#ConfigMap
kubectl get configmaps
kubectl create configmap dbconfig1 --from-literal MYSQL_ROOT_PASSWORD=redhat321
kubectl describe configmaps dbconfig1

#Example
---------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
   run: db1
   name:  db1
spec:
  containers:
  - name: db1
    image: mysql
    env:
    - name: MYSQL_ROOT_PASSWORD
      valueFrom:
        configMapKeyRef:
          name: dbconfig1
          key: MYSQL_ROOT_PASSWORD
-------------------------------------

kubectl describe pods db1 | grep -i configmap
kubectl edit configmaps dbconfig1
kubectl logs pods/db1
kubectl events -w 
kubectl create configmap demo1 --from-literal name=reja --from-literal roll=6024 --from-literal capital=dhaka


#Secrets 
kubectl get secrets
kubectl create secret generic dbsec1 --from-literal mypass=redhat321
kubectl get secrets dbsec1
kubectl describe secrets dbsec1
kubectl get secrets dbsec1 -o yaml 
echo <value> | base64 -d 
kubectl describe pods db1 | grep -i secret 
kubectl describe pods db1 | grep -i secret -C 5

#Example
-----------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
   run: db1
   name:  db1
spec:
  containers:
  - name: db1
    image: mysql
    env:
    - name: MYSQL_ROOT_PASSWORD
      valueFrom:
        secretKeyRef:
          name: dbconfig1
          key: mypass 
----------------------------------------

#Volumes 
   Ephemeral
   Persistent

#Ephemeral
   emptyDir

/var/lib/kubelet  #Storage location
kubectl exec -it multipod -c doc1 -- df -HT
kubectl exec -it multipod -c doc1 -- df -hT
kubectl exec -it pods/multipod --container pod1 -- ls -l /shared
kubectl exec -it pods/multipod --container pod2 -- touch /data/bata.txt
kubectl exec -it pods/multipod --container pod1 -- df -HT
kubectl describe pod multipod | more volume


#Example (single pod)
-------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod1
  name: pod1
spec:
  volumes: 
    - name: cache-volume
      emptyDir:
        sizeLimit: 1Gi

  containers:
  - image: nginx
    name: pod1
    resources: 
      limits: 
        memory: 256Mi
        

    volumeMounts: 
       - name: cache-volume
         mountPath: /cache
-----------------------------------



